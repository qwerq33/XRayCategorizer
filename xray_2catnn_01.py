# -*- coding: utf-8 -*-
"""XRay_2catNN_01.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zX-gG4Lht9UwivE3i_-R9aw7pJ9q8KF-

Coded by Zachary Walker, with help from a lot of tutorials. 

Finds whether an X-ray is sick (with either viral or bacterial pneumonia) vs healthy with 75% accuracy.
"""

import sys

# tensorflow and tqdm come preinstalled in Colab 
# (syntactically where this program is designed to run)
# !pip install tensorflow
!pip install tflearn
#!pip install tqdm

# if this is not the first time running the program in this instance
# you can skip this module

# load Google Drive helper and mount directory
# best way to import image datasets in Colab
# since Colab file storage is dynamic
# you get a new VM every time you open the file
from google.colab import drive

# this will prompt for authentication to get into your Google Drive
drive.mount('/content/drive')

# list all files, just for visual verification
!ls "/content/drive/My Drive/XrayData"

# if this is not the first time running the program in this instance
# you can skip this module

# unzip the archive 
!unzip "/content/drive/My Drive/XrayData/chest_xray_2cat.zip"

import cv2 # for working with images
import numpy as np # for arrays
import os # for directories
from random import shuffle # for shuffling data
from tqdm import tqdm # for visual progress bars
# tqdm not neccessary to run, but is a nice touch

# define directories

TRAIN_DIR = "chest_xray_2cat/train"
TEST_DIR = "chest_xray_2cat/test"
IMG_SIZE = 50 # the size all images will be resized to, in pixels
LR = 1e-3 # Learning rate

# define label_img method 

def label_img(img):
  word_label = img.split('!')[-2]
  
  #convert to one-hot array [healthy, sick]
  
  if word_label == 'heal': return [1,0]
  elif word_label == 'sick': return [0,1]

MODEL_NAME = 'XrayLungs2Cat.model'.format(LR, '2conv-basic')

# define create training data method

def create_train_data():
  training_data = []
  for img in tqdm(os.listdir(TRAIN_DIR)):
    label = label_img(img)
    path = os.path.join(TRAIN_DIR, img)
    img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)
    img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))
    training_data.append([np.array(img), np.array(label)])
  shuffle(training_data)
  np.save('train_data.npy', training_data)
    # saves the training data so if you re-run the model you don't have to recreate it
  return training_data

def process_test_data():
  testing_data = []
  for img in tqdm(os.listdir(TEST_DIR)):
    path = os.path.join(TEST_DIR, img)
    img_num = img.split('!')[0]
    img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)
    img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))
  testing_data.append([np.array(img), img_num])
  
  shuffle(testing_data)
  np.save('test_data.npy', testing_data)
    # saves testing data in case of re-run, same as training
  return testing_data

# import neccessary modules for neural network
import tflearn
from tflearn.layers.conv import conv_2d, max_pool_2d
from tflearn.layers.core import input_data, dropout, fully_connected
from tflearn.layers.estimator import regression

import tensorflow as tf

# create neural network

convnet = input_data(shape = [None, IMG_SIZE, IMG_SIZE, 1], name = 'input')
convnet = conv_2d(convnet, 32, 5, activation = 'relu')
convnet = max_pool_2d(convnet, 5)

convnet = conv_2d(convnet, 64, 5, activation = 'relu')
convnet = max_pool_2d(convnet, 5)

convnet = conv_2d(convnet, 128, 5, activation = 'relu')
convnet = max_pool_2d(convnet, 5)

convnet = conv_2d(convnet, 256, 5, activation = 'relu')
convnet = max_pool_2d(convnet, 5)

convnet = conv_2d(convnet, 128, 5, activation = 'relu')
convnet = max_pool_2d(convnet, 5)

convnet = conv_2d(convnet, 64, 5, activation = 'relu')
convnet = max_pool_2d(convnet, 5)

convnet = conv_2d(convnet, 32, 5, activation = 'relu')
convnet = max_pool_2d(convnet, 5)

convnet = fully_connected(convnet, 1024, activation = 'relu')
convnet = dropout(convnet, 0.8)

convnet = fully_connected(convnet, 2, activation = 'softmax')
convnet = regression(convnet, optimizer = 'adam', learning_rate = LR, loss = 'categorical_crossentropy', name = 'targets')
# use Adam optimizer because it learns along the way

model = tflearn.DNN(convnet, tensorboard_dir='log')

# loads the model if previously saved

if os.path.exists('{}.meta'.format(MODEL_NAME)):
  model.load(MODEL_NAME)
  print('model loaded!')

# create the dataset from the images
train_data = create_train_data()

# separate data
train = train_data[:-500]
test = train_data[-500:]

# load arrays into variables

X = np.array([i[0] for i in train]).reshape(-1, IMG_SIZE, IMG_SIZE, 1)
Y = [i[1] for i in train]

test_x = np.array([i[0] for i in test]).reshape(-1, IMG_SIZE, IMG_SIZE,1)
test_y = [i[1] for i in test]

# fit the model

model.fit({'input':X}, {'targets':Y}, n_epoch = 30, validation_set=({'input':test_x}, {'targets': test_y}),
         snapshot_step=500, show_metric=True, run_id=MODEL_NAME)